{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment Questions\n",
        "1. What is hypothesis testing in statistics.\n",
        " - Statistical hypothesis testing is a method used to determine if there's enough evidence in a sample to draw conclusions about a population, involving formulating hypotheses (null and alternative), collecting data, and assessing the evidence to decide whether to reject or fail to reject the null hypothesis.\n",
        "2. What is the null hypothesis, and how does it differ from the alternative hypothesis.\n",
        " - In hypothesis testing, the null hypothesis (H0) assumes no effect or relationship, while the alternative hypothesis (H1 or Ha) proposes that there is an effect or relationship.\n",
        " Null Hypothesis (H0):\n",
        "Definition:\n",
        "The null hypothesis is a statement of \"no effect\" or \"no difference\" between groups or variables.\n",
        "Purpose:\n",
        "It's the hypothesis that researchers aim to disprove or reject through statistical analysis.\n",
        "Example:\n",
        "In a study comparing two teaching methods, the null hypothesis might state that there is no difference in student performance between the two methods.\n",
        "Symbol:\n",
        "H0\n",
        "Assumptions:\n",
        "The null hypothesis assumes that any observed differences or relationships are due to chance.\n",
        "Alternative Hypothesis (H1 or Ha):\n",
        "Definition:\n",
        "The alternative hypothesis is a statement that contradicts the null hypothesis, suggesting that there is a significant difference or relationship.\n",
        "Purpose:\n",
        "It's the hypothesis that researchers are trying to prove or support through statistical evidence.\n",
        "Example:\n",
        "In the same teaching method study, the alternative hypothesis might state that one teaching method leads to significantly better student performance than the other.\n",
        "Symbol:\n",
        "H1 or Ha\n",
        "Assumptions:\n",
        "The alternative hypothesis assumes that the observed differences or relationships are not due to chance, but rather reflect a real effect.  \n",
        "\n",
        "3. What is the significance level in hypothesis testing, and why is it important.\n",
        " - In hypothesis testing, the significance level (often denoted as Î± or alpha) is the probability of rejecting the null hypothesis when it is actually true, representing the risk of making a Type I error. It's crucial because it sets the threshold for determining statistical significance, guiding researchers in deciding whether to reject or fail to reject the null hypothesis.\n",
        "4. What does a P-value represent in hypothesis testing.\n",
        " - In hypothesis testing, a p-value represents the probability of obtaining results as extreme as, or more extreme than, the observed results, assuming the null hypothesis is true. A smaller p-value indicates stronger evidence against the null hypothesis.\n",
        "Here's a more detailed explanation:\n",
        "Null Hypothesis:\n",
        "In hypothesis testing, the null hypothesis (H0) is a statement that there is no significant difference or effect between groups or variables.\n",
        "P-value:\n",
        "The p-value is calculated based on the assumption that the null hypothesis is true. It quantifies the likelihood of observing the data (or more extreme data) if the null hypothesis were actually true.\n",
        "Interpretation:\n",
        "A small p-value (typically less than 0.05, or 5%) suggests that the observed results are unlikely to have occurred by chance if the null hypothesis is true. This leads to the rejection of the null hypothesis in favor of the alternative hypothesis.\n",
        "A large p-value (greater than 0.05) suggests that the observed results could reasonably be explained by chance, and the null hypothesis is not rejected.\n",
        "Example:\n",
        "If you are testing whether a new drug improves a condition, the null hypothesis might be that the drug has no effect. A p-value of 0.01 would suggest that the observed improvement is unlikely to be due to chance, and the drug might be considered effective.\n",
        "In essence, the p-value helps determine the strength of evidence against the null hypothesis, guiding researchers in deciding whether to reject or fail to reject the null hypothesis based on the observed data.\n",
        "5. How do you interpret the P-value in hypothesis testing.\n",
        " - A p-value less than 0.05 is typically considered to be statistically significant, in which case the null hypothesis should be rejected. A p-value greater than 0.05 means that deviation from the null hypothesis is not statistically significant, and the null hypothesis is not rejected.\n",
        "6. What are Type 1 and Type 2 errors in hypothesis testing.\n",
        " - In hypothesis testing, a Type I error (false positive) occurs when you reject a true null hypothesis, while a Type II error (false negative) occurs when you fail to reject a false null hypothesis.\n",
        "Here's a more detailed explanation:\n",
        "Type I Error (False Positive):\n",
        "This happens when you conclude that there is a significant effect or difference when, in reality, there is none.\n",
        "You incorrectly reject the null hypothesis (which is actually true).\n",
        "Think of it as concluding someone is guilty when they are actually innocent.\n",
        "Type II Error (False Negative):\n",
        "This happens when you fail to detect a real effect or difference that actually exists.\n",
        "You incorrectly fail to reject the null hypothesis (which is actually false).\n",
        "Think of it as concluding someone is innocent when they are actually guilty.\n",
        "7. What is the difference between a one-tailed and a two-tailed test in hypothesis testing.\n",
        " - In hypothesis testing, a one-tailed test checks for a difference in one specific direction (e.g., greater than or less than), while a two-tailed test checks for a difference in either direction (greater or less than).\n",
        "Here's a more detailed explanation:\n",
        "One-tailed test:\n",
        "Used when the alternative hypothesis specifies a direction of the effect (e.g., \"the new drug will reduce blood pressure,\" or \"the new fertilizer will increase crop yield\").\n",
        "The critical region (where the null hypothesis is rejected) is located in one tail of the distribution.\n",
        "Example: Testing whether a new treatment is better than the current treatment, rather than just different.\n",
        "Two-tailed test:\n",
        "Used when the alternative hypothesis does not specify a direction (e.g., \"the new drug will have an effect on blood pressure,\" or \"the new fertilizer will have an effect on crop yield\").\n",
        "The critical region is located in both tails of the distribution.\n",
        "Example: Testing whether a new treatment is different from the current treatment, without specifying whether it's better or worse.\n",
        "8. What is the Z-test, and when is it used in hypothesis testing.\n",
        " - A Z-test is a statistical test used to determine if there's a significant difference between a sample mean and a population mean or between the means of two populations when the population standard deviation is known or the sample size is large, typically greater than 30.\n",
        "Here's a more detailed explanation:\n",
        "What is a Z-test?\n",
        "Purpose:\n",
        "The Z-test is a hypothesis test that helps determine if a sample's mean is significantly different from a hypothesized population mean or if two population means are different.\n",
        "Assumptions:\n",
        "The population standard deviation is known, or the sample size is large (n > 30).\n",
        "The data is normally distributed, or the sample size is large enough that the Central Limit Theorem applies.\n",
        "How it works:\n",
        "The Z-test calculates a Z-statistic, which represents how many standard deviations the sample mean is away from the hypothesized population mean.\n",
        "This Z-statistic is then compared to a critical value or used to calculate a p-value to determine if the observed difference is statistically significant.\n",
        "Types of Z-tests:\n",
        "One-sample Z-test: Compares a sample mean to a known population mean.\n",
        "Two-sample Z-test: Compares the means of two independent samples.\n",
        "Z-test for proportions: Used to compare the proportions of two groups.\n",
        "9. How do you calculate the Z-score, and what does it represent in hypothesis testing.\n",
        " - To calculate a z-score, subtract the population mean from a raw score and divide by the population standard deviation, representing how many standard deviations a data point is from the mean. In hypothesis testing, z-scores are used to determine if a sample's mean is significantly different from a population mean.\n",
        "10. What is the T-distribution, and when should it be used instead of the normal distribution.\n",
        " - The t-distribution, also known as Student's t-distribution, is a probability distribution similar to the normal distribution but with heavier tails, used when estimating population parameters from small samples or when the population standard deviation is unknown. You should use the t-distribution instead of the normal distribution when the sample size is small (typically less than 30) and the population standard deviation is unknown.\n",
        "11. What is the difference between a Z-test and a T-test.\n",
        " - The primary difference between a Z-test and a T-test lies in their application: Z-tests are used with large sample sizes (n > 30) and known population standard deviation, while T-tests are used with small sample sizes (n < 30) and unknown population standard deviation.\n",
        "Here's a more detailed breakdown:\n",
        "Z-Test:\n",
        "Purpose: Used to determine if the mean of a population differs significantly from a hypothesized value or another population mean.\n",
        "Sample Size: Typically used when the sample size is large (n > 30).\n",
        "Population Standard Deviation: Assumes the population standard deviation is known.\n",
        "Distribution: Relies on the normal distribution.\n",
        "When to use: When you have a large sample size and know the population standard deviation.\n",
        "T-Test:\n",
        "Purpose:\n",
        "Used to compare the means of two groups or to determine if the mean of a sample differs significantly from a known value.\n",
        "Sample Size:\n",
        "Often used when the sample size is small (n < 30).\n",
        "Population Standard Deviation:\n",
        "Assumes the population standard deviation is unknown and estimates it from the sample data.\n",
        "Distribution:\n",
        "Relies on the Student's t-distribution, which accounts for the uncertainty in estimating the population standard deviation.\n",
        "When to use:\n",
        "When you have a small sample size and do not know the population standard deviation.\n",
        "12. What is the T-test, and how is it used in hypothesis testing.\n",
        " - A t-test is an inferential statistic used to determine if there is a significant difference between the means of two groups and how they are related. T-tests are used when the data sets follow a normal distribution and have unknown variances, like the data set recorded from flipping a coin 100 times.\n",
        "13. What is the relationship between Z-test and T-test in hypothesis testing.\n",
        " - Key Differences Between Z-Test Vs T-Test | SimplilearnZ-tests and t-tests are both used in hypothesis testing to determine if there's a statistically significant difference between groups or variables, but they differ in their assumptions about the population standard deviation and sample size. Z-tests are used when the population standard deviation is known, or when the sample size is large (n > 30), while t-tests are used when the population standard deviation is unknown, or when the sample size is small (n < 30).\n",
        "14. What is a confidence interval, and how is it used to interpret statistical results.\n",
        " - A confidence interval is a range of values that is likely to contain the true population parameter with a specified level of confidence, typically 95% or 99%. It's used to interpret statistical results by quantifying the uncertainty around a sample estimate and providing a range of plausible values for the population parameter.\n",
        "15. What is the margin of error, and how does it affect the confidence interval.\n",
        " - The margin of error (MOE) is a statistical measure indicating the precision of an estimate, representing half the width of a confidence interval. A smaller MOE implies a more precise estimate, while a larger MOE indicates a wider range of uncertainty.\n",
        "16. How is Bayes' Theorem used in statistics, and what is its significance.\n",
        " - Bayes' Theorem, a fundamental concept in statistics and probability, allows for updating the probability of a hypothesis based on new evidence, transitioning from prior to posterior probabilities. Its significance lies in its ability to model uncertainty and make more informed decisions based on incomplete information.\n",
        "17. What is the Chi-square distribution, and when is it used.\n",
        " - The chi-square distribution is a continuous probability distribution used in statistical hypothesis testing, particularly for analyzing categorical data, with two common applications being the chi-square goodness-of-fit test and the chi-square test of independence.\n",
        "18. What is the Chi-square goodness of fit test, and how is it applied.\n",
        " - The chi-square goodness-of-fit test determines if a sample data distribution matches a theoretical distribution by comparing observed and expected frequencies, and is used to assess how well a model fits the data.\n",
        "19. What is the F-distribution, and when is it used in hypothesis testing.\n",
        " - The F-distribution is a continuous probability distribution used in hypothesis testing, particularly for comparing variances of two or more populations, and is commonly used in the F-test and ANOVA (Analysis of Variance).\n",
        "20. What is an ANOVA test, and what are its assumptions.\n",
        " - ANOVA (Analysis of Variance) is a statistical test used to compare the means of three or more groups, determining if there are significant differences between them, while its key assumptions are normality, independence of observations, and homogeneity of variance.\n",
        "21. What are the different types of ANOVA tests.\n",
        " - The main types of ANOVA tests are one-way ANOVA, two-way ANOVA, and repeated measures ANOVA, with one-way ANOVA comparing means of three or more independent groups, two-way ANOVA examining two independent variables and their interaction, and repeated measures ANOVA comparing means of the same group at different time points.\n",
        "22. What is the F-test, and how does it relate to hypothesis testing?\n",
        " - The F-test, a statistical test used in hypothesis testing, compares variances between two or more groups, using the F-statistic, which follows an F-distribution, to determine if there's a significant difference in the means of those groups.\n",
        "\n",
        "# Practical Questions\n",
        "                "
      ],
      "metadata": {
        "id": "RwDQ0IwcLnLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Write a Python program to perform a Z-test for comparing a sample mean to a known population mean and interpret the results.\n",
        "\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# Sample data\n",
        "sample_data = [10, 12, 11, 13, 10, 9, 12, 11, 10, 11]\n",
        "\n",
        "# Population parameters\n",
        "population_mean = 10\n",
        "population_std = 1.5\n",
        "\n",
        "# Calculate sample statistics\n",
        "sample_mean = np.mean(sample_data)\n",
        "sample_size = len(sample_data)\n",
        "\n",
        "# Calculate the Z-statistic\n",
        "z_statistic = (sample_mean - population_mean) / (population_std / np.sqrt(sample_size))\n",
        "\n",
        "# Calculate the p-value (two-tailed test)\n",
        "p_value = 2 * (1 - stats.norm.cdf(abs(z_statistic)))\n",
        "\n",
        "# Set significance level (alpha)\n",
        "alpha = 0.05\n",
        "\n",
        "# Interpret the results\n",
        "print(\"Z-statistic:\", z_statistic)\n",
        "print(\"P-value:\", p_value)\n",
        "\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis.\")\n",
        "    print(\"There is a significant difference between the sample mean and the population mean.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis.\")\n",
        "    print(\"There is no significant difference between the sample mean and the population mean.\")\n",
        "# 2. Simulate random data to perform hypothesis testing and calculate the corresponding P-value using Python.\n",
        " import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# Simulate two random samples\n",
        "np.random.seed(0)  # for reproducibility\n",
        "sample1 = np.random.normal(loc=10, scale=2, size=100)\n",
        "sample2 = np.random.normal(loc=10.5, scale=2, size=100)\n",
        "\n",
        "# Perform an independent two-sample t-test\n",
        "t_statistic, p_value = stats.ttest_ind(sample1, sample2)\n",
        "\n",
        "# Print the results\n",
        "print(\"T-statistic:\", t_statistic)\n",
        "print(\"P-value:\", p_value)\n",
        "\n",
        "# Set significance level (alpha)\n",
        "alpha = 0.05\n",
        "\n",
        "# Make a decision\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis\")\n",
        "#3. Implement a one-sample Z-test using Python to compare the sample mean with the population mean.\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "def one_sample_ztest(sample_data, pop_mean, pop_std, alpha=0.05, alternative='two-sided'):\n",
        "    \"\"\"\n",
        "    Performs a one-sample Z-test.\n",
        "\n",
        "    Args:\n",
        "        sample_data (list or np.array): The sample data.\n",
        "        pop_mean (float): The population mean.\n",
        "        pop_std (float): The population standard deviation.\n",
        "        alpha (float, optional): The significance level. Defaults to 0.05.\n",
        "        alternative (str, optional): Defines the alternative hypothesis.\n",
        "                                     Must be one of {'two-sided', 'less', 'greater'}.\n",
        "                                     Defaults to 'two-sided'.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the z-statistic and the p-value.\n",
        "    \"\"\"\n",
        "    n = len(sample_data)\n",
        "    sample_mean = np.mean(sample_data)\n",
        "    z_stat = (sample_mean - pop_mean) / (pop_std / np.sqrt(n))\n",
        "\n",
        "    if alternative == 'two-sided':\n",
        "        p_value = 2 * stats.norm.cdf(-np.abs(z_stat))\n",
        "    elif alternative == 'less':\n",
        "        p_value = stats.norm.cdf(z_stat)\n",
        "    elif alternative == 'greater':\n",
        "        p_value = 1 - stats.norm.cdf(z_stat)\n",
        "    else:\n",
        "         raise ValueError(\"alternative must be one of {'two-sided', 'less', 'greater'}\")\n",
        "\n",
        "    return z_stat, p_value\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Example usage\n",
        "    sample = [85, 90, 78, 92, 88, 83, 89, 95, 80, 86]\n",
        "    population_mean = 82\n",
        "    population_std_dev = 8\n",
        "\n",
        "    z_statistic, p_value = one_sample_ztest(sample, population_mean, population_std_dev)\n",
        "\n",
        "    print(\"Z-statistic:\", z_statistic)\n",
        "    print(\"P-value:\", p_value)\n",
        "\n",
        "    alpha = 0.05\n",
        "    if p_value < alpha:\n",
        "        print(\"Reject the null hypothesis\")\n",
        "    else:\n",
        "        print(\"Fail to reject the null hypothesis\")\n",
        "# 4. Perform a two-tailed Z-test using Python and visualize the decision region on a plot.\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "\n",
        "# Parameters\n",
        "alpha = 0.05  # Significance level\n",
        "mean_null = 0  # Null hypothesis mean\n",
        "std_dev = 1    # Population standard deviation\n",
        "sample_size = 50\n",
        "mean_sample = 0.2 # Sample mean (replace with your actual sample mean)\n",
        "\n",
        "# Calculate the Z-score\n",
        "z_score = (mean_sample - mean_null) / (std_dev / np.sqrt(sample_size))\n",
        "\n",
        "# Calculate critical values for a two-tailed test\n",
        "critical_value_left = norm.ppf(alpha/2)\n",
        "critical_value_right = norm.ppf(1 - alpha/2)\n",
        "\n",
        "# Calculate p-value\n",
        "p_value = 2 * (1 - norm.cdf(np.abs(z_score)))\n",
        "\n",
        "#Decision\n",
        "reject_null = (z_score < critical_value_left) or (z_score > critical_value_right)\n",
        "\n",
        "#Output\n",
        "print(f\"Z-score: {z_score:.3f}\")\n",
        "print(f\"P-value: {p_value:.3f}\")\n",
        "print(f\"Critical values: {critical_value_left:.3f}, {critical_value_right:.3f}\")\n",
        "print(f\"Reject null hypothesis: {reject_null}\")\n",
        "\n",
        "# Visualize the decision region\n",
        "x = np.linspace(-4, 4, 200)\n",
        "y = norm.pdf(x, 0, 1)\n",
        "\n",
        "plt.plot(x, y, label='Standard Normal Distribution')\n",
        "plt.axvline(critical_value_left, color='r', linestyle='--', label=f'Critical Value Left ({critical_value_left:.2f})')\n",
        "plt.axvline(critical_value_right, color='r', linestyle='--', label=f'Critical Value Right ({critical_value_right:.2f})')\n",
        "plt.axvline(z_score, color='g', linestyle='-', label=f'Z-score ({z_score:.2f})')\n",
        "\n",
        "# Shade the rejection regions\n",
        "x_fill_left = np.linspace(-4, critical_value_left, 100)\n",
        "y_fill_left = norm.pdf(x_fill_left, 0, 1)\n",
        "plt.fill_between(x_fill_left, y_fill_left, color='r', alpha=0.3)\n",
        "\n",
        "x_fill_right = np.linspace(critical_value_right, 4, 100)\n",
        "y_fill_right = norm.pdf(x_fill_right, 0, 1)\n",
        "plt.fill_between(x_fill_right, y_fill_right, color='r', alpha=0.3)\n",
        "\n",
        "\n",
        "plt.title('Two-Tailed Z-Test Decision Region')\n",
        "plt.xlabel('Z-score')\n",
        "plt.ylabel('Probability Density')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "# 5. Create a Python function that calculates and visualizes Type 1 and Type 2 errors during hypothesis testing\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "\n",
        "def visualize_hypothesis_testing(null_mean, alternative_mean, std_dev, alpha, sample_size):\n",
        "    \"\"\"\n",
        "    Calculates and visualizes Type 1 and Type 2 errors in hypothesis testing.\n",
        "\n",
        "    Args:\n",
        "        null_mean (float): Mean of the null hypothesis distribution.\n",
        "        alternative_mean (float): Mean of the alternative hypothesis distribution.\n",
        "        std_dev (float): Standard deviation of both distributions.\n",
        "        alpha (float): Significance level (probability of Type 1 error).\n",
        "        sample_size (int): Sample size.\n",
        "    \"\"\"\n",
        "\n",
        "    sem = std_dev / np.sqrt(sample_size)\n",
        "    z_critical = norm.ppf(1 - alpha)\n",
        "    critical_value = null_mean + z_critical * sem\n",
        "\n",
        "    beta = norm.cdf(critical_value, loc=alternative_mean, scale=sem)\n",
        "    power = 1 - beta\n",
        "\n",
        "    x = np.linspace(null_mean - 4 * sem, alternative_mean + 4 * sem, 200)\n",
        "    y_null = norm.pdf(x, null_mean, sem)\n",
        "    y_alternative = norm.pdf(x, alternative_mean, sem)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(x, y_null, label='Null Hypothesis', color='blue')\n",
        "    plt.plot(x, y_alternative, label='Alternative Hypothesis', color='green')\n",
        "\n",
        "    plt.axvline(critical_value, color='red', linestyle='--', label=f'Critical Value: {critical_value:.2f}')\n",
        "\n",
        "    x_fill_type1 = np.linspace(critical_value, null_mean + 4 * sem, 100)\n",
        "    y_fill_type1 = norm.pdf(x_fill_type1, null_mean, sem)\n",
        "    plt.fill_between(x_fill_type1, y_fill_type1, color='red', alpha=0.3, label=f'Type 1 Error (alpha): {alpha:.2f}')\n",
        "\n",
        "    x_fill_type2 = np.linspace(null_mean - 4 * sem, critical_value, 100)\n",
        "    y_fill_type2 = norm.pdf(x_fill_type2, alternative_mean, sem)\n",
        "    plt.fill_between(x_fill_type2, y_fill_type2, color='orange', alpha=0.3, label=f'Type 2 Error (beta): {beta:.2f}')\n",
        "\n",
        "    plt.xlabel('Sample Mean')\n",
        "    plt.ylabel('Probability Density')\n",
        "    plt.title('Hypothesis Testing Visualization')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Critical Value: {critical_value:.2f}\")\n",
        "    print(f\"Type 1 Error (alpha): {alpha:.2f}\")\n",
        "    print(f\"Type 2 Error (beta): {beta:.2f}\")\n",
        "    print(f\"Power (1 - beta): {power:.2f}\")\n",
        "#6. Write a Python program to perform an independent T-test and interpret the results.\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# Sample data for two independent groups\n",
        "group1 = np.array([78, 85, 69, 92, 88, 81, 75, 95])\n",
        "group2 = np.array([65, 72, 58, 80, 75, 68, 70, 78])\n",
        "\n",
        "# Perform independent T-test\n",
        "t_statistic, p_value = stats.ttest_ind(group1, group2)\n",
        "\n",
        "# Set significance level (alpha)\n",
        "alpha = 0.05\n",
        "\n",
        "# Print results\n",
        "print(\"T-statistic:\", t_statistic)\n",
        "print(\"P-value:\", p_value)\n",
        "\n",
        "# Interpret results\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis.\")\n",
        "    print(\"There is a significant difference between the means of the two groups.\")\n",
        "else:\n",
        "    print(\"Fail to the null hypothesis.\")\n",
        "    print(\"There is no significant difference between the means of the two groups.\")\n",
        "#7. Perform a paired sample T-test using Python and visualize the comparison results.\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sample data for before and after measurements\n",
        "before = [78, 85, 69, 92, 88, 81, 75, 95]\n",
        "after = [82, 88, 72, 95, 90, 85, 78, 98]\n",
        "\n",
        "# Perform paired sample T-test\n",
        "t_statistic, p_value = stats.ttest_rel(before, after)\n",
        "\n",
        "# Print results\n",
        "print(\"T-statistic:\", t_statistic)\n",
        "print(\"P-value:\", p_value)\n",
        "\n",
        "# Set significance level (alpha)\n",
        "alpha = 0.05\n",
        "\n",
        "# Interpret results\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis.\")\n",
        "    print(\"There is a significant difference between the before and after measurements.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis.\")\n",
        "    print(\"There is no significant difference between the before and after measurements.\")\n",
        "\n",
        "# Visualize comparison\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(before, after, 'o', color='blue', label='Data Points')\n",
        "plt.xlabel(\"Before Measurement\")\n",
        "plt.ylabel(\"After Measurement\")\n",
        "plt.title(\"Paired Sample T-test Comparison\")\n",
        "\n",
        "# Add a diagonal line for reference\n",
        "plt.plot([min(before), max(before)], [min(before), max(before)], linestyle='--', color='gray', label='No Difference Line')\n",
        "#8. Simulate data and perform both Z-test and T-test, then compare the results using Python.\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "from statsmodels.stats.weightstats import ztest\n",
        "\n",
        "# Simulate data\n",
        "np.random.seed(0)\n",
        "sample_size = 50\n",
        "population_mean = 100\n",
        "population_std = 15\n",
        "sample = np.random.normal(population_mean, population_std, sample_size)\n",
        "\n",
        "# Z-test\n",
        "z_statistic, z_p_value = ztest(sample, value=population_mean)\n",
        "print(\"Z-test results:\")\n",
        "print(\"Z-statistic:\", z_statistic)\n",
        "print(\"P-value:\", z_p_value)\n",
        "\n",
        "# T-test\n",
        "t_statistic, t_p_value = stats.ttest_1samp(sample, population_mean)\n",
        "print(\"\\nT-test results:\")\n",
        "print(\"T-statistic:\", t_statistic)\n",
        "print(\"P-value:\", t_p_value)\n",
        "\n",
        "# Comparison\n",
        "print(\"\\nComparison:\")\n",
        "print(\"Are p-values similar?\", np.isclose(z_p_value, t_p_value))\n",
        "print(\"Are test statistics similar?\", np.isclose(z_statistic, t_statistic))\n",
        "#9. Write a Python function to calculate the confidence interval for a sample mean and explain its significance.\n",
        "import numpy as np\n",
        "import scipy.stats as st\n",
        "\n",
        "def confidence_interval(data, confidence_level=0.95):\n",
        "    \"\"\"\n",
        "    Calculates the confidence interval for a sample mean.\n",
        "\n",
        "    Args:\n",
        "        data: A list or array of sample data.\n",
        "        confidence_level: The desired confidence level (e.g., 0.95 for 95% confidence).\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the lower and upper bounds of the confidence interval.\n",
        "    \"\"\"\n",
        "    n = len(data)\n",
        "    mean = np.mean(data)\n",
        "    std_err = st.sem(data)  # Standard error of the mean\n",
        "\n",
        "    # Calculate the critical value (t-score)\n",
        "    t_critical = st.t.ppf((1 + confidence_level) / 2, n - 1)\n",
        "\n",
        "    # Calculate the margin of error\n",
        "    margin_of_error = t_critical * std_err\n",
        "\n",
        "    # Calculate the confidence interval\n",
        "    lower_bound = mean - margin_of_error\n",
        "    upper_bound = mean + margin_of_error\n",
        "\n",
        "    return lower_bound, upper_bound\n",
        "# 10. Write a Python program to calculate the margin of error for a given confidence level using sample data\n",
        "import numpy as np\n",
        "import scipy.stats as st\n",
        "\n",
        "def calculate_margin_of_error(data, confidence=0.95):\n",
        "    \"\"\"\n",
        "    Calculates the margin of error for a given confidence level.\n",
        "\n",
        "    Args:\n",
        "        data (list or numpy.ndarray): Sample data.\n",
        "        confidence (float, optional): Confidence level (e.g., 0.95 for 95%).\n",
        "                                     Defaults to 0.95.\n",
        "\n",
        "    Returns:\n",
        "        float: Margin of error.\n",
        "    \"\"\"\n",
        "    sample_mean = np.mean(data)\n",
        "    sample_std = np.std(data, ddof=1)  # ddof=1 for sample standard deviation\n",
        "    sample_size = len(data)\n",
        "\n",
        "    # Calculate t-critical value\n",
        "    t_critical = st.t.ppf((1 + confidence) / 2, df=sample_size - 1)\n",
        "\n",
        "    # Calculate standard error\n",
        "    standard_error = sample_std / np.sqrt(sample_size)\n",
        "\n",
        "    # Calculate margin of error\n",
        "    margin_of_error = t_critical * standard_error\n",
        "\n",
        "    return margin_of_error\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    data = [23, 25, 28, 30, 32, 35, 37, 39]\n",
        "    confidence_level = 0.95\n",
        "\n",
        "    margin = calculate_margin_of_error(data, confidence_level)\n",
        "    print(f\"Margin of error for {confidence_level*100}% confidence level: {margin:.2f}\")\n",
        "\n",
        "    confidence_level_99 = 0.99\n",
        "    margin_99 = calculate_margin_of_error(data, confidence_level_99)\n",
        "    print(f\"Margin of error for {confidence_level_99*100}% confidence level: {margin_99:.2f}\")\n",
        "#11. Implement a Bayesian inference method using Bayes' Theorem in Python and explain the process\n",
        "import numpy as np\n",
        "\n",
        "def bayes_theorem(prior_probability, likelihood, evidence):\n",
        "  \"\"\"\n",
        "  Calculates the posterior probability using Bayes' Theorem.\n",
        "\n",
        "  Args:\n",
        "    prior_probability: The probability of the hypothesis before observing evidence.\n",
        "    likelihood: The probability of the evidence given the hypothesis.\n",
        "    evidence: The probability of the evidence.\n",
        "\n",
        "  Returns:\n",
        "    The posterior probability.\n",
        "  \"\"\"\n",
        "  return (likelihood * prior_probability) / evidence\n",
        "\n",
        "# Example Usage (using hypothetical data)\n",
        "# Assume we have a hypothesis: \"It's raining\"\n",
        "# Prior belief: We think there's a 40% chance of rain\n",
        "prior_probability = 0.4\n",
        "\n",
        "# Evidence: We hear thunder\n",
        "# Likelihood: Given it's raining, there's a 90% chance we hear thunder\n",
        "# We can define the likelihood as:\n",
        "likelihood_given_rain = 0.9\n",
        "\n",
        "# Probability of evidence (thunder)\n",
        "evidence_probability = 0.3\n",
        "\n",
        "# Update belief using Bayes' theorem\n",
        "posterior_probability = bayes_theorem(prior_probability, likelihood_given_rain, evidence_probability)\n",
        "\n",
        "print(f\"Posterior Probability: {posterior_probability}\")\n",
        "#12. D Perform a Chi-square test for independence between two categorical variables in Python\n",
        "import pandas as pd\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Sample data (replace with your actual data)\n",
        "data = {'Gender': ['Male', 'Female', 'Male', 'Female', 'Male', 'Female'],\n",
        "        'Smoker': ['Yes', 'No', 'No', 'Yes', 'Yes', 'No']}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Create contingency table\n",
        "contingency_table = pd.crosstab(df['Gender'], df['Smoker'])\n",
        "print(\"Contingency Table:\")\n",
        "print(contingency_table)\n",
        "\n",
        "# Perform Chi-square test\n",
        "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
        "\n",
        "print(\"\\nChi-square statistic:\", chi2)\n",
        "print(\"P-value:\", p)\n",
        "print(\"Degrees of freedom:\", dof)\n",
        "print(\"Expected frequencies:\\n\", expected)\n",
        "\n",
        "# Interpret the results\n",
        "alpha = 0.05\n",
        "if p < alpha:\n",
        "    print(\"\\nReject the null hypothesis: There is a significant association between Gender and Smoker.\")\n",
        "else:\n",
        "    print(\"\\nFail to reject the null hypothesis: There is no significant association between Gender and Smoker.\")\n",
        "#13.Write a Python program to calculate the expected frequencies for a Chi-square test based on observed data\n",
        "import numpy as np\n",
        "\n",
        "def calculate_expected_frequencies(observed_data):\n",
        "    \"\"\"\n",
        "    Calculates the expected frequencies for a Chi-square test.\n",
        "\n",
        "    Args:\n",
        "        observed_data (list or numpy.ndarray): A 1D or 2D array containing the observed frequencies.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: An array containing the expected frequencies.\n",
        "    \"\"\"\n",
        "    observed_data = np.array(observed_data)\n",
        "    if observed_data.ndim == 1:  # For one-way Chi-square\n",
        "        total_sum = np.sum(observed_data)\n",
        "        num_categories = len(observed_data)\n",
        "        expected_frequencies = np.full(num_categories, total_sum / num_categories)\n",
        "    elif observed_data.ndim == 2:  # For two-way Chi-square\n",
        "        row_totals = observed_data.sum(axis=1, keepdims=True)\n",
        "        col_totals = observed_data.sum(axis=0, keepdims=True)\n",
        "        grand_total = observed_data.sum()\n",
        "        expected_frequencies = (row_totals @ col_totals) / grand_total\n",
        "    else:\n",
        "        raise ValueError(\"Observed data must be 1D or 2D array.\")\n",
        "    return expected_frequencies\n",
        "#14. D Perform a goodness-of-fit test using Python to compare the observed data to an expected distribution\n",
        "import numpy as np\n",
        "from scipy.stats import chisquare\n",
        "\n",
        "# Sample observed frequencies\n",
        "observed_frequencies = [10, 20, 30, 40, 25]\n",
        "\n",
        "# Sample expected frequencies (e.g., based on a uniform distribution)\n",
        "expected_frequencies = [25, 25, 25, 25, 25]\n",
        "\n",
        "# Perform the chi-square test\n",
        "chi2_statistic, p_value = chisquare(observed_frequencies, expected_frequencies)\n",
        "\n",
        "# Print the results\n",
        "print(\"Chi-square statistic:\", chi2_statistic)\n",
        "print(\"P-value:\", p_value)\n",
        "\n",
        "# Choose a significance level\n",
        "alpha = 0.05\n",
        "\n",
        "# Interpret the results\n",
        "if p_value <= alpha:\n",
        "    print(\"Reject the null hypothesis. The observed data significantly deviates from the expected distribution.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis. The observed data is consistent with the expected distribution.\")\n",
        "#15. Create a Python script to simulate and visualize the Chi-square distribution and discuss its characteristics\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import chi2\n",
        "\n",
        "# Parameters for the Chi-square distribution\n",
        "degrees_of_freedom = [1, 3, 5, 10]  # Different degrees of freedom to visualize\n",
        "\n",
        "# Generate x-values for the plot\n",
        "x = np.linspace(0, 20, 500)\n",
        "\n",
        "# Plot the Chi-square distributions\n",
        "plt.figure(figsize=(10, 6))\n",
        "for df in degrees_of_freedom:\n",
        "    y = chi2.pdf(x, df)  # Probability density function\n",
        "    plt.plot(x, y, label=f'df = {df}')\n",
        "\n",
        "# Customize the plot\n",
        "plt.title('Chi-Square Distribution')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('Probability Density')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.show()\n",
        "#16.  Implement an F-test using Python to compare the variances of two random samples\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "def f_test(sample1, sample2, alpha=0.05):\n",
        "    \"\"\"\n",
        "    Performs an F-test to compare the variances of two samples.\n",
        "\n",
        "    Args:\n",
        "        sample1 (array-like): First sample data.\n",
        "        sample2 (array-like): Second sample data.\n",
        "        alpha (float, optional): Significance level. Defaults to 0.05.\n",
        "\n",
        "    Returns:\n",
        "        tuple: F-statistic, p-value, and a boolean indicating whether to reject the null hypothesis.\n",
        "    \"\"\"\n",
        "    # Calculate variances and degrees of freedom\n",
        "    var1 = np.var(sample1, ddof=1)\n",
        "    var2 = nporr.var(sample2, ddof=1)\n",
        "    df1 = len(sample1) - 1\n",
        "    df2 = len(sample2) - 1\n",
        "\n",
        "    # Calculate F-statistic\n",
        "    if var1 > var2:\n",
        "        f_statistic = var1 / var2\n",
        "    else:\n",
        "        f_statistic = var2 / var1\n",
        "\n",
        "    # Calculate p-value (two-tailed)\n",
        "    p_value = 2 * min(stats.f.cdf(f_statistic, df1, df2), 1 - stats.f.cdf(f_statistic, df1, df2))\n",
        "\n",
        "    # Determine if the null hypothesis should be rejected\n",
        "    reject_null = p_value < alpha\n",
        "\n",
        "    return f_statistic, p_value, reject_null\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Example usage\n",
        "    np.random.seed(0)  # for reproducibility\n",
        "    sample_a = np.random.normal(0, 2, 100)\n",
        "    sample_b = np.random.normal(0, 3, 100)\n",
        "\n",
        "    f_stat, p_val, reject = f_test(sample_a, sample_b)\n",
        "\n",
        "    print(\"F-statistic:\", f_stat)\n",
        "    print(\"P-value:\", p_val)\n",
        "    print(\"Reject null hypothesis:\", reject)\n",
        "\n",
        "    if reject:\n",
        "        print(\"The variances are significantly different.\")\n",
        "    else:\n",
        "        print(\"There is no significant difference in variances.\")\n",
        "#17. Write a Python program to perform an ANOVA test to compare means between multiple groups and interpret the results\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# Create sample data\n",
        "np.random.seed(42)\n",
        "group1 = np.random.normal(loc=30, scale=5, size=50)\n",
        "group2 = np.random.normal(loc=35, scale=5, size=50)\n",
        "group3 = np.random.normal(loc=40, scale=5, size=50)\n",
        "\n",
        "# Create a pandas DataFrame\n",
        "data = pd.DataFrame({'value': np.concatenate([group1, group2, group3]),\n",
        "                     'group': np.repeat(['Group1', 'Group2', 'Group3'], 50)})\n",
        "\n",
        "# Perform ANOVA test\n",
        "f_statistic, p_value = stats.f_oneway(data['value'][data['group'] == 'Group1'],\n",
        "                                        data['value'][data['group'] == 'Group2'],\n",
        "                                        data['value'][data['group'] == 'Group3'])\n",
        "\n",
        "# Print results\n",
        "print(\"F-statistic:\", f_statistic)\n",
        "print(\"P-value:\", p_value)\n",
        "\n",
        "# Interpret results\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"The null hypothesis is rejected. There is a significant difference between the means of at least two groups.\")\n",
        "else:\n",
        "    print(\"The null hypothesis is not rejected. There is no significant difference between the means of the groups.\")\n",
        "#18.Perform a one-way ANOVA test using Python to compare the means of different groups and plot the results\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Sample data (replace with your actual data)\n",
        "data = {'Group': np.repeat(['A', 'B', 'C'], 10),\n",
        "        'Value': np.concatenate([np.random.normal(10, 2, 10),\n",
        "                                   np.random.normal(12, 2, 10),\n",
        "                                   np.random.normal(15, 2, 10)])}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Perform one-way ANOVA\n",
        "fvalue, pvalue = stats.f_oneway(df['Value'][df['Group'] == 'A'],\n",
        "                                df['Value'][df['Group'] == 'B'],\n",
        "                                df['Value'][df['Group'] == 'C'])\n",
        "\n",
        "print(f\"F-value: {fvalue}, P-value: {pvalue}\")\n",
        "\n",
        "# Plot the data and ANOVA results\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.boxplot(x='Group', y='Value', data=df)\n",
        "sns.swarmplot(x='Group', y='Value', data=df, color='black', alpha=0.4) # Overlay data points\n",
        "plt.title(f'One-way ANOVA Results (F={fvalue:.2f}, p={pvalue:.3f})')\n",
        "\n",
        "# Annotate the plot with significance\n",
        "if pvalue < 0.05:\n",
        "    plt.text(1, df['Value'].max() * 0.9, 'Significant difference', ha='center', color='red')\n",
        "else:\n",
        "    plt.text(1, df['Value'].max() * 0.9, 'No significant difference', ha='center', color='green')\n",
        "\n",
        "plt.show()\n",
        "#19.Write a Python function to check the assumptions (normality, independence, and equal variance) for ANOVA\n",
        "import scipy.stats as stats\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def check_anova_assumptions(data, groups):\n",
        "    \"\"\"\n",
        "    Checks the assumptions of normality, independence, and equal variance for ANOVA.\n",
        "\n",
        "    Args:\n",
        "        data (pd.Series): The dependent variable data.\n",
        "        groups (pd.Series): The independent variable (groups) data.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the results of the assumption tests.\n",
        "    \"\"\"\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # 1. Normality (Shapiro-Wilk test for residuals)\n",
        "    # First, we need a model to get residuals\n",
        "    model = stats.statsmodels.formula.api.ols('data ~ groups', data=pd.DataFrame({'data':data, 'groups':groups})).fit() # Fit a simple linear model\n",
        "    residuals = model.resid\n",
        "\n",
        "    # Perform Shapiro-Wilk test on residuals\n",
        "    shapiro_test = stats.shapiro(residuals)\n",
        "    results['normality'] = {\n",
        "        'test': 'Shapiro-Wilk',\n",
        "        'statistic': shapiro_test.statistic,\n",
        "        'p_value': shapiro_test.pvalue\n",
        "    }\n",
        "\n",
        "    # 2. Independence (Not easily testable without more data)\n",
        "    #   Independence is difficult to test in a simple setting. We assume it if the\n",
        "    #   data collection method is appropriate. For example, if data are collected randomly from a population,\n",
        "    #   then we might assume that the data are independent.\n",
        "    results['independence'] = 'Assumed to be independent based on data collection method.'\n",
        "\n",
        "    # 3. Equal Variance (Levene's test)\n",
        "    # Use Levene's test for homogeneity of variance\n",
        "    levene_test = stats.levene(data[groups == groups.unique()[0]], data[groups == groups.unique()[1]]) # Test between two groups\n",
        "    results['equal_variance'] = {\n",
        "        'test': 'Levene\\'s',\n",
        "        'statistic': levene_test.statistic,\n",
        "        'p_value': levene_test.pvalue\n",
        "    }\n",
        "\n",
        "    return results\n",
        "\n",
        "# Example usage (replace with your actual data)\n",
        "# Create some sample data\n",
        "data = pd.Series(np.random.normal(0, 1, 100))\n",
        "groups = pd.Series(np.random.choice(['A', 'B'], 100))\n",
        "\n",
        "# Check assumptions\n",
        "assumption_results = check_anova_assumptions(data, groups)\n",
        "print(assumption_results)\n",
        "#20.Perform a two-way ANOVA test using Python to study the interaction between two factors and visualize the results\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Sample Data (replace with your actual data)\n",
        "data = {'FactorA': ['A1', 'A1', 'A1', 'A2', 'A2', 'A2', 'A1', 'A1', 'A1', 'A2', 'A2', 'A2'],\n",
        "        'FactorB': ['B1', 'B1', 'B1', 'B1', 'B1', 'B1', 'B2', 'B2', 'B2', 'B2', 'B2', 'B2'],\n",
        "        'Value': [10, 12, 15, 20, 25, 18, 18, 20, 22, 30, 35, 28]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Two-way ANOVA\n",
        "model = ols('Value ~ C(FactorA) + C(FactorB) + C(FactorA):C(FactorB)', data=df).fit()\n",
        "anova_table = sm.stats.anova_lm(model, typ=2)\n",
        "print(anova_table)\n",
        "\n",
        "# Visualization\n",
        "sns.catplot(x='FactorA', y='Value', hue='FactorB', data=df, kind='bar')\n",
        "plt.title('Interaction Plot of Factor A and Factor B')\n",
        "plt.show()\n",
        "#21.Write a Python program to visualize the F-distribution and discuss its use in hypothesis testing\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import f\n",
        "\n",
        "# Define the degrees of freedom\n",
        "dfn = 5  # numerator degrees of freedom\n",
        "dfd = 20 # denominator degrees of freedom\n",
        "\n",
        "# Generate x values\n",
        "x = np.linspace(0, 5, 500)\n",
        "\n",
        "# Calculate the probability density function (PDF)\n",
        "pdf = f.pdf(x, dfn, dfd)\n",
        "\n",
        "# Plot the F-distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(x, pdf, label=f'F({dfn}, {dfd})')\n",
        "plt.title('F-Distribution')\n",
        "plt.xlabel('F-value')\n",
        "plt.ylabel('Probability Density')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "#22. Perform a one-way ANOVA test in Python and visualize the results with boxplots to compare group means\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Sample data (replace with your actual data)\n",
        "data = {'Group': np.repeat(['A', 'B', 'C'], 10),\n",
        "        'Value': np.concatenate([np.random.normal(10, 2, 10),\n",
        "                                   np.random.normal(12, 2, 10),\n",
        "                                   np.random.normal(15, 2, 10)])}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Perform one-way ANOVA\n",
        "fvalue, pvalue = stats.f_oneway(df['Value'][df['Group'] == 'A'],\n",
        "                                df['Value'][df['Group'] == 'B'],\n",
        "                                df['Value'][df['Group'] == 'C'])\n",
        "\n",
        "print(f\"F-value: {fvalue}, P-value: {pvalue}\")\n",
        "\n",
        "# Plot the data and ANOVA results\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.boxplot(x='Group', y='Value', data=df)\n",
        "sns.swarmplot(x='Group', y='Value', data=df, color='black', alpha=0.4) # Overlay data points\n",
        "plt.title(f'One-way ANOVA Results (F={fvalue:.2f}, p={pvalue:.3f})')\n",
        "\n",
        "# Annotate the plot with significance\n",
        "if pvalue < 0.05:\n",
        "    plt.text(1, df['Value'].max() * 0.9, 'Significant difference', ha='center', color='red')\n",
        "else:\n",
        "    plt.text(1, df['Value'].max() * 0.9, 'No significant difference', ha='center', color='green')\n",
        "\n",
        "plt.show()\n",
        "#23. Simulate random data from a normal distribution, then perform hypothesis testing to evaluate the means\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# Parameters for the normal distribution\n",
        "mu = 10  # Mean\n",
        "sigma = 2  # Standard deviation\n",
        "sample_size = 30\n",
        "\n",
        "# Generate random data\n",
        "data = np.random.normal(mu, sigma, sample_size)\n",
        "\n",
        "# Hypothesis testing\n",
        "# Null hypothesis: The population mean is equal to mu (mu = 10)\n",
        "# Alternative hypothesis: The population mean is not equal to mu (mu != 10)\n",
        "\n",
        "# Perform one-sample t-test\n",
        "t_statistic, p_value = stats.ttest_1samp(data, mu)\n",
        "\n",
        "# Set significance level (alpha)\n",
        "alpha = 0.05\n",
        "\n",
        "# Print results\n",
        "print(\"T-statistic:\", t_statistic)\n",
        "print(\"P-value:\", p_value)\n",
        "\n",
        "# Interpret results\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis.\")\n",
        "    print(\"There is a significant difference between the sample mean and the hypothesized population mean.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis.\")\n",
        "    print(\"There is no significant difference between the sample mean and the hypothesized population mean.\")\n",
        "#24.Perform a hypothesis test for population variance using a Chi-square distribution and interpret the results\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# Sample data\n",
        "data = [12, 15, 18, 14, 16, 13, 17, 15, 19, 16]\n",
        "\n",
        "# Hypothesized population variance\n",
        "hypothesized_variance = 4\n",
        "\n",
        "# Significance level\n",
        "alpha = 0.05\n",
        "\n",
        "# Calculate sample statistics\n",
        "sample_size = len(data)\n",
        "sample_variance = np.var(data, ddof=1)  # ddof=1 for unbiased sample variance\n",
        "\n",
        "# Calculate the Chi-square statistic\n",
        "chi_square_statistic = (sample_size - 1) * sample_variance / hypothesized_variance\n",
        "\n",
        "# Calculate the p-value (two-tailed test)\n",
        "p_value = 2 * min(stats.chi2.cdf(chi_square_statistic, df=sample_size - 1),\n",
        "                  1 - stats.chi2.cdf(chi_square_statistic, df=sample_size - 1))\n",
        "\n",
        "# Print results\n",
        "print(\"Chi-square statistic:\", chi_square_statistic)\n",
        "print(\"P-value:\", p_value)\n",
        "\n",
        "# Interpret results\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis.\")\n",
        "    print(\"There is a significant difference between the sample variance and the hypothesized population variance.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis.\")\n",
        "    print(\"There is no significant difference between the sample variance and the hypothesized population variance.\")\n",
        "#25.Write a Python script to perform a Z-test for comparing proportions between two datasets or groups\n",
        "import statsmodels.api as sm\n",
        "import numpy as np\n",
        "\n",
        "def z_test_proportions(successes1, observations1, successes2, observations2, alpha=0.05, alternative='two-sided'):\n",
        "    \"\"\"\n",
        "    Performs a two-sample z-test for proportions.\n",
        "\n",
        "    Parameters:\n",
        "    successes1 (int): Number of successes in the first group.\n",
        "    observations1 (int): Total number of observations in the first group.\n",
        "    successes2 (int): Number of successes in the second group.\n",
        "    observations2 (int): Total number of observations in the second group.\n",
        "    alpha (float, optional): Significance level. Defaults to 0.05.\n",
        "    alternative (str, optional): 'two-sided', 'one-sided-less', or 'one-sided-greater'.\n",
        "                                 Defaults to 'two-sided'.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing the z-statistic and the p-value.\n",
        "    \"\"\"\n",
        "    if alternative not in ['two-sided', 'one-sided-less', 'one-sided-greater']:\n",
        "        raise ValueError(\"alternative must be 'two-sided', 'one-sided-less', or 'one-sided-greater'\")\n",
        "\n",
        "    z_stat, p_value = sm.stats.proportions_ztest([successes1, successes2], [observations1, observations2], alternative=alternative)\n",
        "    return z_stat, p_value\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Example usage:\n",
        "    successes_A = 65\n",
        "    observations_A = 200\n",
        "    successes_B = 80\n",
        "    observations_B = 250\n",
        "    alpha = 0.05\n",
        "\n",
        "    z_statistic, p_value = z_test_proportions(successes_A, observations_A, successes_B, observations_B, alpha=alpha, alternative='two-sided')\n",
        "\n",
        "    print(f\"Z-statistic: {z_statistic:.3f}\")\n",
        "    print(f\"P-value: {p_value:.3f}\")\n",
        "\n",
        "    if p_value < alpha:\n",
        "        print(\"Reject the null hypothesis: There is a significant difference in proportions.\")\n",
        "    else:\n",
        "        print(\"Fail to reject the null hypothesis: There is no significant difference in proportions.\")\n",
        "#26. Implement an F-test for comparing the variances of two datasets, then interpret and visualize the results\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate two datasets (replace with your actual data)\n",
        "np.random.seed(123)  # for reproducibility\n",
        "data1 = np.random.normal(loc=10, scale=2, size=30)\n",
        "data2 = np.random.normal(loc=12, scale=3, size=40)\n",
        "\n",
        "# Perform F-test\n",
        "fvalue, pvalue = stats.f_oneway(data1, data2)\n",
        "# stats.f_oneway is used for comparing variances when the sample sizes are unequal\n",
        "# In this case, it's still applicable\n",
        "\n",
        "print(\"F-value:\", fvalue)\n",
        "print(\"P-value:\", pvalue)\n",
        "\n",
        "# Interpretation\n",
        "alpha = 0.05  # Significance level\n",
        "if pvalue < alpha:\n",
        "    print(\"Reject the null hypothesis.\")\n",
        "    print(\"The variances of the two datasets are significantly different.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis.\")\n",
        "    print(\"There is no significant difference between the variances of the two datasets.\")\n",
        "\n",
        "# Visualization: Box plots to compare distributions\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.boxplot([data1, data2], labels=['Dataset 1', 'Dataset 2'])\n",
        "plt.title('Comparison of Variances (F-test)')\n",
        "plt.ylabel('Values')\n",
        "plt.show()\n",
        "#27.Perform a Chi-square test for goodness of fit with simulated data and analyze the results.\n",
        "import random\n",
        "import numpy as np\n",
        "import scipy.stats\n",
        "\n",
        "# Simulate data for a categorical variable (e.g., coin flips)\n",
        "# Assuming a 50/50 probability of heads or tails, we simulate 100 coin flips\n",
        "num_simulations = 100\n",
        "observed_frequencies = []\n",
        "for i in range(num_simulations):\n",
        "    coin_flips = [random.choice(['heads', 'tails']) for _ in range(100)]\n",
        "    heads_count = coin_flips.count('heads')\n",
        "    observed_frequencies.append(heads_count)\n",
        "\n",
        "print(f\"Simulated observed frequencies (first 10): {observed_frequencies[:10]}\")\n",
        "\n",
        "# Expected frequencies (assuming a 50/50 distribution for heads and tails)\n",
        "expected_frequency = 100 * 0.5  # Expected number of heads in 100 coin flips\n",
        "expected_frequencies = [expected_frequency] * num_simulations\n",
        "# Print expected frequencies for the first 10 simulations.\n",
        "print(f\"Simulated expected frequencies (first 10): {expected_frequencies[:10]}\")\n",
        "Chi-Square Goodness-of-Fit Test:\n",
        "State the null and alternative hypotheses:\n",
        "Null hypothesis (H0): The observed frequencies are consistent with the assumed distribution.\n",
        "Alternative hypothesis (H1): The observed frequencies are significantly different from the assumed distribution.\n",
        "Significance level is set (usually 0.05)\n",
        "Calculate the Chi-square statistic:\n",
        "Python\n",
        "\n",
        "    # Perform chi-square test for goodness of fit\n",
        "    chi2_statistic = 0\n",
        "    for observed, expected in zip(observed_frequencies, expected_frequencies):\n",
        "        chi2_statistic += ((observed - expected)**2) / expected\n",
        "\n",
        "    print(f\"Calculated Chi-square statistic: {chi2_statistic}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ImyVAmGqQ9pq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}